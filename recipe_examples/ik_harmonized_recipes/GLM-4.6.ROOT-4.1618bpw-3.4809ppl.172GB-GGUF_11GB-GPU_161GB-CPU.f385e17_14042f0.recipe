## Quant mix recipe created using Thireus' GGUF Tool Suite - https://gguf.thireus.com/
# Model name: GLM-4.6
# Link to the original model: https://huggingface.co/zai-org/GLM-4.6

## Model head & embeddings — qbits: 32 8 
^output_norm\.weight$=f32
^token_embd\.weight$=q8_0
^output\.weight$=q8_0

## Multi-headed attention parameters — qbits: 32 5 
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_norm\.weight$=f32
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_output\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_k\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_v\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_k_norm\.weight$=f32
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_v\.bias$=f32
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_q\.bias$=f32
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_q_norm\.weight$=f32
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_k\.bias$=f32
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.attn_q\.weight$=iq5_ks_r4

## Dense Feed-Forward Network weights — qbits: 8 5 
^blk\.[0-2]\.ffn_down\.weight$=iq5_ks_r4
^blk\.2\.ffn_up\.weight$=iq5_ks_r4
^blk\.[0-1]\.ffn_up\.weight$=q8_0

## NextN tensors — qbits: 32 5 
^blk\.92\.nextn\.enorm\.weight$=f32
^blk\.92\.nextn\.eh_proj\.weight$=iq5_ks_r4
^blk\.92\.nextn\.shared_head_norm\.weight$=f32
^blk\.92\.nextn\.hnorm\.weight$=f32

## MoE Gating & Routing — qbits: 32 
^blk\.([3-9]|[1-8][0-9]|9[0-2])\.ffn_gate_inp\.weight$=f32
^blk\.([3-9]|[1-8][0-9]|9[0-2])\.exp_probs_b\.bias$=f32

## Gating network — qbits: 6 5 
^blk\.0\.ffn_gate\.weight$=iq5_ks_r4
^blk\.[1-2]\.ffn_gate\.weight$=iq6_k

## Misc / Other tensors — qbits: 32 
^blk\.([0-9]|[1-8][0-9]|9[0-2])\.post_attention_norm\.weight$=f32

## GPU-loaded - MoE Shared Experts Feed-Forward Network - ffn_*_shexp
# ffn_down_shexp — down-projection (shared experts) — qbits: 8 6 5 
^blk\.(3|8|15|20|23|2[5-6]|30|34|84|91)\.ffn_down_shexp\.weight$=q8_0
^blk\.([4-7]|9|10|1[7-9]|21|4[4-9]|[5-6][0-1]|53|5[5-7]|59|6[3-5]|67|69|7[0-6]|78|80|8[2-3]|85|88|92)\.ffn_down_shexp\.weight$=iq6_k
^blk\.(1[1-4]|16|22|24|2[7-9]|3[1-3]|3[5-9]|4[0-3]|52|54|58|62|66|68|77|79|81|8[6-7]|89|90)\.ffn_down_shexp\.weight$=iq5_ks_r4

# ffn_up_shexp — up-projection (shared experts) — qbits: 8 6 5 
^blk\.(3|7|11|20|22|2[4-6]|2[8-9]|3[1-3]|39|4[3-4]|48|86|89|9[0-1])\.ffn_up_shexp\.weight$=q8_0
^blk\.([5-6]|[8-9]|10|1[2-3]|15|19|21|23|3[4-8]|40|42|4[6-7]|49|5[0-9]|60|6[2-9]|7[1-9]|80|82|8[4-5]|87|92)\.ffn_up_shexp\.weight$=iq6_k
^blk\.(4|14|1[6-8]|27|30|41|45|61|70|81|83|88)\.ffn_up_shexp\.weight$=iq5_ks_r4

# ffn_gate_shexp — gating network (shared experts) — qbits: 8 6 5 
^blk\.([3-5]|13|21|2[4-9]|3[6-7]|46|8[4-8]|9[0-1])\.ffn_gate_shexp\.weight$=q8_0
^blk\.([8-9]|1[0-1]|1[5-6]|19|23|35|4[1-5]|4[8-9]|5[0-9]|6[0-3]|6[5-9]|7[0-4]|7[6-7]|79|8[0-3]|89|92)\.ffn_gate_shexp\.weight$=iq6_k
^blk\.([6-7]|12|14|1[7-8]|20|22|3[0-4]|3[8-9]|40|47|64|75|78)\.ffn_gate_shexp\.weight$=iq5_ks_r4

## CPU-friendly - MoE Per-expert Feed-Forward Network - ffn_*_exps
# ffn_down_exps — down-projection (per-expert) — qbits: 5 4 3 
^blk\.(11|20|22|2[6-9]|3[0-2]|39|4[2-5]|4[7-8]|5[2-3]|5[5-9]|6[0-1]|6[3-5]|68|7[1-3]|77|79)\.ffn_down_exps\.weight$=iq5_ks_r4
^blk\.(3|8|12|1[4-9]|2[3-5]|33|3[6-8]|[4-5][0-1]|46|49|54|62|6[6-7]|69|70|7[4-6]|78|8[0-9]|9[0-2])\.ffn_down_exps\.weight$=iq4_kt
^blk\.([4-7]|9|10|13|21|3[4-5])\.ffn_down_exps\.weight$=iq3_kt

# ffn_up_exps — up-projection (per-expert) — qbits: 5 4 3 
^blk\.([4-5][8-9]|6[4-6]|68|7[0-2])\.ffn_up_exps\.weight$=iq5_ks_r4
^blk\.(4|9|13|1[5-8]|22|24|28|30|32|34|3[7-8]|4[1-2]|4[4-7]|5[0-7]|6[0-3]|67|69|7[3-9]|8[0-9]|9[0-2])\.ffn_up_exps\.weight$=iq4_kt
^blk\.(3|[5-8]|1[0-2]|14|19|2[0-1]|23|2[5-7]|29|31|33|3[5-6]|39|40|43)\.ffn_up_exps\.weight$=iq3_kt

# ffn_gate_exps — gating network (per-expert) — qbits: 5 4 3 
^blk\.([4-5][8-9]|6[4-6]|68|7[0-2])\.ffn_gate_exps\.weight$=iq5_ks_r4
^blk\.(4|9|13|1[5-8]|22|24|28|30|32|34|3[7-8]|4[1-2]|4[4-7]|5[0-7]|6[0-3]|67|69|7[3-9]|8[0-9]|9[0-2])\.ffn_gate_exps\.weight$=iq4_kt
^blk\.(3|[5-8]|1[0-2]|14|19|2[0-1]|23|2[5-7]|29|31|33|3[5-6]|39|40|43)\.ffn_gate_exps\.weight$=iq3_kt

## Summary of tensor sizes per class
# GPU Total: 11.655 GiB (95.1%) | 12.26 GiB max, if all were q8_0 | 10.95 GiB min, if all were iq5_ks_r4
# CPU Total: 161.206 GiB (77.6%) | 207.64 GiB max, if all were iq5_ks_r4 | 123.60 GiB min, if all were iq3_kt
# GPU+CPU Total: 172.861 GiB (86.3%)

## Summary of tensor counts and bpw per qtype
#
# GPU-loaded quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# +f32       	835	32.0  	  0.28 GiB	-		-
# +q8_0      	1  	8.5   	  0.77 GiB	-		-
# q8_0      	56 	8.5   	  1.30 GiB	38.0%		3.43
# iq6_k     	150	6.625 	  0.99 GiB	37.2%		2.67
# +iq5_ks_r4 	373	5.25  	  7.78 GiB	-		-
# iq5_ks_r4 	74 	5.25  	  0.52 GiB	24.7%		2.12
#
# CPU-friendly quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# iq5_ks_r4 	57 	5.25  	 43.84 GiB	21.1%		207.64
# iq4_kt    	155	4.0   	 90.82 GiB	57.4%		158.20
# iq3_kt    	58 	3.125 	 26.55 GiB	21.5%		123.60
#
# -Average BPW: 4.1618
#
# -Notes:
# - '+' means user-defined pre-assigned tensors, or tensor missing from csv data or f32 tensors
# - Recipe produced on the 2025-10-05 18:56:10 UTC+0000 using Thireus' GGUF tools (https://gguf.thireus.com/)
# - Script SHA-256: f385e17ea9998140203cc543e8e9d3635f6f1292999c58d837cc6d2d3d48b1e0
# - Calibration dataset 'ppl_results.csv' SHA-256: 5d15ad4864a06be06db0a1e8d9fe479b475293dbf858edcf44c6e8427a7e8232
# - tensors.bf16.map SHA-256: fa987db60ed8e9eb4348a45cb0ad630f81a97b20292ed9adfe0369ddd3ec2828
# - tensors.bf16.map model name: GLM-4.6-THIREUS-BF16-SPECIAL_TENSOR-01760-of-01760
# - tensors.iq4_kt.map SHA-256: a12c3663545b2e1d6cefe13c2d1b1a24ee5d6a6cba9b05539b9b92e0d185a56a
# - tensors.iq4_kt.map model name: GLM-4.6-THIREUS-IQ4_KT-SPECIAL_TENSOR-01760-of-01760
# - tensors.iq5_ks_r4.map SHA-256: 5c570d0b404729a5ec8d359471ac60edd86cfac80cad58999b596fcf0e61fbb0
# - tensors.iq5_ks_r4.map model name: GLM-4.6-THIREUS-IQ5_KS_R4-SPECIAL_TENSOR-01760-of-01760
# - tensors.iq3_kt.map SHA-256: 27a5950da62f69b54f524c5e4a37057c8b114fcc4eca736d615613a98282a0dd
# - tensors.iq3_kt.map model name: GLM-4.6-THIREUS-IQ3_KT-SPECIAL_TENSOR-01760-of-01760
# - tensors.iq6_k.map SHA-256: 0743f08065ebeeac64c52844c0a1cbde4e4e9242230d4218de7647fd46d2ba99
# - tensors.iq6_k.map model name: GLM-4.6-THIREUS-IQ6_K-SPECIAL_TENSOR-01760-of-01760
# - tensors.q8_0.map SHA-256: 4cdc6924a3e79e3a8df15cc40d607d01ad2d29375eb411ca423a44d437ae0c66
# - tensors.q8_0.map model name: GLM-4.6-THIREUS-Q8_0-SPECIAL_TENSOR-01760-of-01760
# - tensors.iq1_kt.map SHA-256: 793fce90c8b4c735e406f9ee701fc10d9e483d90fbdafdab533096ac7d9e748e
# - tensors.iq1_kt.map model name: GLM-4.6-THIREUS-IQ1_KT-SPECIAL_TENSOR-01760-of-01760
# - GPG signatures: PASSED
# - Command used:
# ../../quant_assign.py ppl_results.csv --tolerance 0.01 --cpu-irq-k 1.5 --gpu-irq-k 1.5 --gpu-assign-qtype iq5_ks_r4 \
# --cpu-tensors-max-size 160 --gpu-tensors-max-size 95% --exponential-factor 8 --cpu-tensors \
# 'blk\.([3-9]|[1-8][0-9]|9[0-2])\.ffn_down_exps\.weight' 'blk\.([3-9]|[1-8][0-9]|9[0-2])\.ffn_up_exps\.weight' \
# 'blk\.([3-9]|[1-8][0-9]|9[0-2])\.ffn_gate_exps\.weight' --gpu-tensors '.*' --cpu-quants iq4_kt iq5_ks_r4 iq3_kt \
# --gpu-quants iq6_k iq5_ks_r4 q8_0 --gpu-assign-tensors 'output\.weight=q8_0' --harmonize-tensors \
# 'blk\..*\.ffn_up_exps.*,blk\..*\.ffn_gate_exps.*' --harmonization-technique 3

## THE END!
