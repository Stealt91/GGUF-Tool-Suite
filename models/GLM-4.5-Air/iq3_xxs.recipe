## Model head & embeddings
token_embd\.weight=q8_0
output\.weight=q8_0
output_norm\.weight=f32

## Multi-headed attention parameters
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.bias=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_output\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.bias=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.bias=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_norm\.weight=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.weight=q8_0

## Core FFN weights
blk\.0\.ffn_gate\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.ffn_norm\.weight=f32
blk\.0\.ffn_down\.weight=q8_0
blk\.0\.ffn_up\.weight=q8_0
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_inp\.bias=f32
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_inp\.weight=f32

## Other tensors
blk\.46\.enorm=f32
blk\.46\.shared_head\.head=q8_0
blk\.46\.embed_tokens=q8_0
blk\.46\.eh_proj=q8_0
blk\.46\.hnorm=f32
blk\.46\.shared_head\.norm=f32

## GPU-loaded ffn_*_shexp
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_shexp\.weight=iq3_xxs
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_up_shexp\.weight=iq3_xxs
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_shexp\.weight=iq3_xxs

## CPU-loaded ffn_*_exps
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight=iq3_xxs
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_up_exps\.weight=iq3_xxs
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_exps\.weight=iq3_xxs
