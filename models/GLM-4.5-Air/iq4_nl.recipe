## Model head & embeddings
token_embd\.weight=q8_0
output\.weight=q8_0
output_norm\.weight=f32

## Multi-headed attention parameters
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.bias=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_output\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.bias=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.weight=q8_0
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.bias=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_norm\.weight=f32
blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.weight=q8_0

## Core FFN weights
blk\.0\.ffn_gate\.weight=q8_0
blk\.0\.ffn_down\.weight=q8_0
blk\.0\.ffn_up\.weight=q8_0
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_inp\.weight=f32

## Other tensors
blk\.([0-9]|[1-3][0-9]|4[0-6])\.post_attention_norm\.weight=f32
blk\.46\.nextn\.shared_head_norm\.weight=f32
blk\.([1-9]|[1-3][0-9]|4[0-6])\.exp_probs_b\.bias=f32
blk\.46\.nextn\.eh_proj\.weight=q8_0
blk\.46\.nextn\.embed_tokens\.weight=q8_0
blk\.46\.nextn\.enorm\.weight=f32
blk\.46\.nextn\.hnorm\.weight=f32
blk\.46\.nextn\.shared_head_head\.weight=q8_0

## GPU-loaded ffn_*_shexp
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_shexp\.weight=iq4_nl
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_up_shexp\.weight=iq4_nl
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_shexp\.weight=iq4_nl

## CPU-loaded ffn_*_exps
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight=iq4_nl
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_up_exps\.weight=iq4_nl
blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_exps\.weight=iq4_nl