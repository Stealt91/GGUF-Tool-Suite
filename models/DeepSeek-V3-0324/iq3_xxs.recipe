## Model head & embeddings
token_embd\.weight=q8_0
output\.weight=q8_0
output_norm\.weight=f32

## Special attention kernels — single-quant only (llama-quantize takes care of it)
blk\.([0-9]|[1-5][0-9]|60)\.attn_k_b\.weight=q8_0

## Multi-headed attention parameters
blk\.([0-9]|[1-5][0-9]|60)\.attn_v_b\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|60)\.attn_kv_a_norm\.weight=f32
blk\.([0-9]|[1-5][0-9]|60)\.attn_kv_a_mqa\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|60)\.attn_output\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|60)\.attn_q_a_norm\.weight=f32
blk\.([0-9]|[1-5][0-9]|60)\.attn_q_b\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|60)\.attn_kv_b\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|60)\.attn_norm\.weight=f32
blk\.([0-9]|[1-5][0-9]|60)\.attn_q_a\.weight=q8_0

## Core FFN weights
blk\.[0-2]\.ffn_gate\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|60)\.ffn_norm\.weight=f32
blk\.[0-2]\.ffn_down\.weight=q8_0
blk\.[0-2]\.ffn_up\.weight=q8_0
blk\.([3-9]|[1-5][0-9]|60)\.ffn_gate_inp\.weight=f32

## Other tensors — qbits: 32 
blk\.([3-9]|[1-5][0-9]|60)\.exp_probs_b\.bias=f32

## GPU-loaded ffn_*_shexp
# ffn_down_shexp (down-projection)
blk\.([3-9]|[1-5][0-9]|60)\.ffn_down_shexp\.weight=q8_0

# ffn_up_shexp (up-projection)
blk\.([3-9]|[1-5][0-9]|60)\.ffn_up_shexp\.weight=q8_0

# ffn_gate_shexp (gate-projection)
blk\.([3-9]|[1-5][0-9]|60)\.ffn_gate_shexp\.weight=q8_0

## CPU-loaded ffn_*_exps
# ffn_down_exps (down-extraction)
blk\.([3-9]|[1-5][0-9]|60)\.ffn_down_exps\.weight=iq3_xxs

# ffn_up_exps (up-extraction)
blk\.([3-9]|[1-5][0-9]|60)\.ffn_up_exps\.weight=iq3_xxs

# ffn_gate_exps (gate-extraction)
blk\.([3-9]|[1-5][0-9]|60)\.ffn_gate_exps\.weight=iq3_xxs