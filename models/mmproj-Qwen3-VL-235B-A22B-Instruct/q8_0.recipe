## Multi-headed attention parameters
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_k\.weight$=q8_0
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_q\.weight$=q8_0
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_k\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_out\.weight$=q8_0
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_v\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_v\.weight$=q8_0
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_q\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.attn_out\.bias$=f32

## Dense Feed-Forward Network weights
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ffn_up\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ffn_down\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ffn_up\.weight$=q8_0
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ffn_down\.weight$=q8_0

## LayerNorm / Post-LN parameters
^v\.post_ln\.bias$=f32
^v\.post_ln\.weight$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ln1\.weight$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ln2\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ln1\.bias$=f32
^v\.blk\.([0-9]|1[0-9]|2[0-6])\.ln2\.weight$=f32

## Embeddings & positional encodings
^v\.patch_embd\.weight$=f32
^v\.patch_embd\.weight\.1$=f32
^v\.position_embd\.weight$=f32

## Deepstack modules
^v\.deepstack\.[0-2]\.fc1\.bias$=f32
^v\.deepstack\.[0-2]\.fc1\.weight$=q8_0
^v\.deepstack\.[0-2]\.norm\.weight$=f32
^v\.deepstack\.[0-2]\.fc2\.weight$=q8_0
^v\.deepstack\.[0-2]\.fc2\.bias$=f32
^v\.deepstack\.[0-2]\.norm\.bias$=f32

## Misc / Other tensors
^mm\.(0|2)\.bias$=f32
^mm\.(0|2)\.weight$=q8_0