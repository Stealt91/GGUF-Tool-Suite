## Model head & embeddings
token_embd\.weight=q8_0
output\.weight=q8_0
output_norm\.weight=f32

## Multi-headed attention parameters
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_v\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_output\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_k_norm\.weight=f32
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_q\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_norm\.weight=f32
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_k\.weight=q8_0
blk\.([0-9]|[1-5][0-9]|6[0-1])\.attn_q_norm\.weight=f32

## Core FFN weights
blk\.([0-9]|[1-5][0-9]|6[0-1])\.ffn_norm\.weight=f32
blk\.([0-9]|[1-5][0-9]|6[0-1])\.ffn_gate_inp\.weight=f32

## CPU-loaded ffn_*_exps
# ffn_down_exps (down-extraction)
blk\.([0-9]|[1-5][0-9]|6[0-1])\.ffn_down_exps\.weight=iq3_xxs

# ffn_up_exps (up-extraction)
blk\.([0-9]|[1-5][0-9]|6[0-1])\.ffn_up_exps\.weight=iq3_xxs

# ffn_gate_exps (gate-extraction)
blk\.([0-9]|[1-5][0-9]|6[0-1])\.ffn_gate_exps\.weight=iq3_xxs
