# --harmonization-technique 1 gives slightly better PPL
## Quant mix recipe created using Thireus' GGUF Tool Suite - https://gguf.thireus.com/
# Model name: GLM-4.5-Air
# Link to the original model: https://huggingface.co/zai-org/GLM-4.5-Air

## Model head & embeddings — qbits: 32 8 5 
^output_norm\.weight$=f32
^token_embd\.weight$=iq5_ks_r4
^output\.weight$=q8_0

## Multi-headed attention parameters — qbits: 32 5 
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.bias$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_output\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.weight$=iq5_ks_r4
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.bias$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.bias$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_norm\.weight$=f32

## Core FFN weights — qbits: 32 8 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_inp\.weight$=f32
^blk\.0\.ffn_down\.weight$=q8_0
^blk\.0\.ffn_gate\.weight$=q8_0
^blk\.0\.ffn_up\.weight$=q8_0

## Other tensors — qbits: 32 5 
^blk\.46\.nextn\.embed_tokens\.weight$=iq5_ks_r4
^blk\.46\.nextn\.shared_head_norm\.weight$=f32
^blk\.46\.nextn\.enorm\.weight$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.post_attention_norm\.weight$=f32
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.exp_probs_b\.bias$=f32
^blk\.46\.nextn\.eh_proj\.weight$=iq5_ks_r4
^blk\.46\.nextn\.shared_head_head\.weight$=iq5_ks_r4
^blk\.46\.nextn\.hnorm\.weight$=f32

## GPU-loaded ffn_*_shexp
# ffn_down_shexp (down-projection) — qbits: 5 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_shexp\.weight$=iq5_ks_r4

# ffn_up_shexp (up-projection) — qbits: 8 6 5 
^blk\.(1|[3-4]|[7-8]|13|17|19|2[3-4]|28|30|34|38|4[1-5])\.ffn_up_shexp\.weight$=q8_0
^blk\.([5-6]|1[0-2]|15|2[0-2]|2[5-7]|29|3[1-3]|3[5-7]|39|40|46)\.ffn_up_shexp\.weight$=iq6_k
^blk\.(2|9|14|16|18)\.ffn_up_shexp\.weight$=iq5_ks_r4

# ffn_gate_shexp (gate-projection) — qbits: 8 6 5 
^blk\.(1|3|[6-9]|1[0-2]|20|23|25|2[7-9]|30|4[0-5])\.ffn_gate_shexp\.weight$=q8_0
^blk\.(2|1[5-6]|1[8-9]|2[1-2]|24|26|3[1-9]|46)\.ffn_gate_shexp\.weight$=iq6_k
^blk\.([4-5]|1[3-4]|17)\.ffn_gate_shexp\.weight$=iq5_ks_r4

## CPU-friendly ffn_*_exps
# ffn_down_exps (down-extraction) — qbits: 4 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight$=iq4_nl

# ffn_up_exps (up-extraction) — qbits: 5 4 3 2 
^blk\.46\.ffn_up_exps\.weight$=iq5_k_r4
^blk\.(2[5-9]|3[0-4]|3[6-9]|41|45)\.ffn_up_exps\.weight$=iq4_ks
^blk\.([2-3]|6|10|13|15|1[8-9]|2[0-4]|35|40|4[2-4])\.ffn_up_exps\.weight$=iq3_k
^blk\.(1|[4-5]|[7-9]|1[1-2]|14|1[6-7])\.ffn_up_exps\.weight$=iq2_ks

# ffn_gate_exps (gate-extraction) — qbits: 5 4 3 2 
^blk\.46\.ffn_gate_exps\.weight$=iq5_k_r4
^blk\.(2[5-9]|3[0-4]|3[6-9]|41|45)\.ffn_gate_exps\.weight$=iq4_ks
^blk\.([2-3]|6|10|13|15|1[8-9]|2[0-4]|35|40|4[2-4])\.ffn_gate_exps\.weight$=iq3_k
^blk\.(1|[4-5]|[7-9]|1[1-2]|14|1[6-7])\.ffn_gate_exps\.weight$=iq2_ks

## Summary of tensor sizes per class
# GPU Total: 5.754 GiB (94.9%) | 6.06 GiB max, if all were q8_0 | 5.36 GiB min, if all were iq5_ks_r4
# CPU Total: 45.192 GiB (87.6%) | 51.61 GiB max, if all were iq4_ks | 35.65 GiB min, if all were iq2_ks
# GPU+CPU Total: 50.946 GiB (91.2%)

## Summary of tensor counts and bpw per qtype
#
# GPU-loaded quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# +f32       	331	32.0  	  0.09 GiB	-		-
# +q8_0      	1  	8.5   	  0.04 GiB	-		-
# q8_0      	44 	8.5   	  0.94 GiB	50.9%		1.84
# iq6_k     	41 	6.625 	  0.18 GiB	12.7%		1.44
# +iq5_ks_r4 	237	5.25  	  4.08 GiB	-		-
# iq5_ks_r4 	11 	5.25  	  0.41 GiB	36.4%		1.14
#
# CPU-friendly quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# +iq5_k_r4  	2  	5.5   	  0.95 GiB	-		-
# +iq4_nl    	46 	4.5   	 17.79 GiB	-		-
# iq4_ks    	32 	4.25  	 11.69 GiB	35.6%		32.87
# iq3_k     	36 	3.4375	 10.63 GiB	40.0%		26.59
# iq2_ks    	22 	2.1875	  4.14 GiB	24.4%		16.92
#
# -Average BPW: 3.9611
#
# -Notes:
# - '+' means user-defined pre-assigned tensors, or tensor missing from csv data or f32 tensors
# - Recipe produced on the 2025-09-02 02:58:57 UTC+0000 using Thireus' GGUF tools (https://gguf.thireus.com/)
# - Script SHA-256: 90e3c2fafeb4aa4360e9fed16bf2aa8f5be90c24da90bd656f38ca48bf77bd1d
# - Calibration dataset 'ppl_results.csv' SHA-256: c596235f01c582988d23f97e1e6809a83923ae3f5321e3cde00625c9c92952f3
# - tensors.bf16.map SHA-256: f440313db9b7ce593240c0b0acb723182ee3ae9570eca868dc6eb440112fdd67
# - tensors.bf16.map model name: GLM-4.5-Air-THIREUS-BF16-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq4_ks.map SHA-256: 06bcfc785b971d87c203f71a2923b64b17df876b36a19b20c5f20056b9cc2e8a
# - tensors.iq4_ks.map model name: GLM-4.5-Air-THIREUS-IQ4_KS-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq3_k.map SHA-256: e6d4287c09a958660da94817eef7a27ab8935937987d5abeca0d98ab67fa0dfc
# - tensors.iq3_k.map model name: GLM-4.5-Air-THIREUS-IQ3_K-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq2_ks.map SHA-256: efed8f3d7d712a6ad99c5904f6e2f4b89387cc78e4008d9ca557bd04da1f2b31
# - tensors.iq2_ks.map model name: GLM-4.5-Air-THIREUS-IQ2_KS-SPECIAL_TENSOR-00804-of-00804
# - tensors.q8_0.map SHA-256: c00093e70a6c32aab72b404457c12a7b238b0e030975267d93d2b09a30796151
# - tensors.q8_0.map model name: GLM-4.5-Air-THIREUS-Q8_0-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq5_ks_r4.map SHA-256: 8a84bd804179f7cb48b50b488321309aed9118a83551d347788d87ae6584be80
# - tensors.iq5_ks_r4.map model name: GLM-4.5-Air-THIREUS-IQ5_KS_R4-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq6_k.map SHA-256: 0da8994b7effbf3f6da93733b58226926d565bf7569f70dd4d0ebca6a6c2fc0a
# - tensors.iq6_k.map model name: GLM-4.5-Air-THIREUS-IQ6_K-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq5_k_r4.map SHA-256: 2314ae706fc4dda3af2a86e917e88039c876376ce6f62c62964f4fbe7eeb8153
# - tensors.iq5_k_r4.map model name: GLM-4.5-Air-THIREUS-IQ5_K_R4-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq4_nl.map SHA-256: ed9419ab49fc319d033148d4241f28fcafb1d3089cb3172da9fa5f62c978a93d
# - tensors.iq4_nl.map model name: GLM-4.5-Air-THIREUS-IQ4_NL-SPECIAL_TENSOR-00804-of-00804
# - GPG signatures: PASSED
# - Command used:
# ../../quant_assign.py ppl_results.csv --tolerance 0.01 --cpu-irq-k 1.5 --gpu-irq-k 1.5 --gpu-assign-qtype iq5_ks_r4 \
# --cpu-tensors-max-size 45 --gpu-tensors-max-size 95% --exponential-factor 8 --cpu-tensors \
# 'blk\.([1-9]|[1-3][0-9]|4[0-5])\.ffn_up_exps\.weight' 'blk\.([1-9]|[1-3][0-9]|4[0-5])\.ffn_gate_exps\.weight' \
# --gpu-tensors '.*' --cpu-quants iq4_ks iq3_k iq2_ks --gpu-quants q8_0 iq5_ks_r4 iq6_k --cpu-assign-tensors \
# 'blk\.(46)\.ffn_up_exps\.weight=iq5_k_r4' 'blk\.(46)\.ffn_gate_exps\.weight=iq5_k_r4' \
# 'blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight=iq4_nl' --gpu-assign-tensors \
# 'blk\.(0)\.ffn_down\.weight=q8_0' --harmonize-tensors 'blk\..*\.ffn_up_exps.*,blk\..*\.ffn_gate_exps.*' \
# --harmonization-technique 3

## THE END!
